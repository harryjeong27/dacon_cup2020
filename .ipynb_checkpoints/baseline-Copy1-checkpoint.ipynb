{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dacon Cup 2020\n",
    "> Analyze and predict user pattern by using user data\n",
    "\n",
    "# Table of Contents\n",
    "1. [Problem Definition & Domain Research](#ProblemDefinition&DomainResearch)  \n",
    "    1.1 [Problem Definiton](#problemdefinition)  \n",
    "    1.2 [Data](#data)  \n",
    "    1.3 [Domain Research & Questions](#domain)  \n",
    "2. [Acquire training and testig data : Data Loading](#dataloading)  \n",
    "    2.1 [Package Loading & Basic Setting](#package)  \n",
    "    2.2 [Data Loading](#loading)  \n",
    "3. [Data Analyze (EDA) & Preprocessing (Wrangle, Cleanse)](#EDA&Wrangling)  \n",
    "    3.1 [Analyze by describing data (Quick-view)¶](#quick)  \n",
    "    3.2 [Assumption in 5-fundamental ways](#assumption)  \n",
    "    3.3 [Analyze by pivoting features](#pivoting)  \n",
    "    3.4 [Analyze by visualizing data in 5 ways](#visual)  \n",
    "    3.5 [Wrangle data](#wrangle)  \n",
    "4. [Modeling, Predict and Solve the problem](#modeling)  \n",
    "    4.1 [Listing possible model](#modellisting)  \n",
    "    4.2 [Modeling](#predicting)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Problem Definition & Domain Research<a name=\"ProblemDefinition&DomainResearch\"></a>\n",
    "## 1.1 Problem Definiton<a name=\"problemdefinition\"></a>\n",
    "### 1.1.1 Topic\n",
    "### 1.1.2 Background\n",
    "### 1.1.3 Purpose \n",
    "### 1.1.4 Host  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data<a name=\"data\"></a>\n",
    "### 1.2.1 Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Domain Research & Questions<a name=\"domain\"></a>\n",
    "- What is Disspent?\n",
    "- Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Acquire training and testig data : Data Loading¶<a name=\"dataloading\"></a>\n",
    "## 2.1 Package Loading & Basic Setting<a name=\"package\"></a>\n",
    "## 2.2 Data Loading<a name=\"loading\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Analyze (EDA) & Preprocessing (Wrangle, Cleanse)<a name=\"EDA&Wrangling\"></a>\n",
    "## 3.1 Analyze by describing data (Quick-view)¶<a name=\"quick\"></a>\n",
    "### 3.1.1 Check columns (name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Check feature type\n",
    "1) Categorical\n",
    "- Categorical\n",
    "- Ordinal\n",
    "2) Numerical\n",
    "- Continous\n",
    "- Discrete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Check errors or typos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Check blank, null or empty values & data types\n",
    "- integer or floats or strings(objects)\n",
    "    df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 Check distribution of numerical feature values\n",
    "    df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Assumption in 5-fundamental ways<a name=\"assumption\"></a>\n",
    ">We arrive at following assumptions based on data analysis done so far. We may validate these assumptions further before taking appropriate actions.\n",
    "\n",
    "### 3.2.1 Correlating.\n",
    "Correlating. One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a correlation among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\n",
    "- correlation btw dependant variable and each explanatory variable\n",
    "\n",
    "### 3.2.2 Completing.\n",
    "Completing. Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\n",
    "- ex) there is no missing values\n",
    "\n",
    "### 3.2.3 Correcting.\n",
    "Correcting. We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\n",
    "- ex) Q_Ques may be dropped as it contains relative number of time (each Q_Time's total is same) & it is hard to find the relation btw Answering time & Reliability.\n",
    "- ex) W_Ques may be dropped as we could not find any relation with voted or other features.\n",
    "- ex) some of features in human group may be dropped as it does not have any relation with voted or others: hand, engnat, familysize.\n",
    "\n",
    "### 3.2.4 Creating.\n",
    "Creating. Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\n",
    "- ex) We may create a new feature called mach_score based on the concept of the maki test.\n",
    "- ex) We may create a new feature called tp_score based on the concpet of TIPI test.\n",
    "\n",
    "### 3.2.5 Classifying.\n",
    "Classifying. We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\n",
    "- ex) 10s are more likely not to have voted. (under the voting age)\n",
    "- ex) The educated are more likely to have voted.\n",
    "- ex) The people with High mach_score are more likely to have voted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Analyze by pivoting features<a name=\"pivoting\"></a>\n",
    "### Dependant variable vs each Explanatory variable\n",
    "- To confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other.\n",
    "- ex) We can only do so at this stage for features which do not have any empty values.\n",
    "It also makes sense doing so only for features which are categorical (human), ordinal (Q_Ques, TP_Ques, age_group) or discrete (familysize) type.\n",
    "\n",
    "### Summary\n",
    "- ex) The voted rate is 0.55. (24898 voted, 20634 not voted out of 45532).\n",
    "- ex) education, age_group, married are strongly related to the voted rate. (classifying)\n",
    "- ex) engnat, gender, hand, race, religion, urban, familysize are not clearly related to the voted rate. (completing for familysize, creating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Analyze by visualizing data in 5 ways<a name=\"visual\"></a>\n",
    ">Confirming some of our assumptions using visualizations for analyzing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.0 Heatmap\n",
    ">Check correlation btw human features\n",
    "- ex) (+) Relation : education & married, voted & married, voted & education\n",
    "- ex) (-) Relation : mach_score & married, mach_score & chin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Correlating based on feature types\n",
    "ex)\n",
    "- mach_score\n",
    "   - 56점 이상 높아지면 점점 not voted가 많아짐 => classifying\n",
    "- tp_score : 큰 의미 없어보임\n",
    "- age_group + mach_score\n",
    "   - 10s are the most, but most did not vote and they tend to have high mach_score\n",
    "   - 40s, 50s, 60s mostly voted and they tend to have relatively low mach_score\n",
    "   - It seems high mach_score provoke low vote rate and the low is opposite.\n",
    "- Married might divide into two groups at [0.0, 1.0], [2.0, )\n",
    "- Education must be an important feature and is already well-grouped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Wrangle data<a name=\"wrangle\"></a>\n",
    ">We have collected several assumptions and decisions regarding our datasets and solution requirements. So far we did not have to change a single feature or value to arrive at these. Let us now execute our decisions and assumptions for correcting, creating, and completing goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 Correcting by dropping features\n",
    "This is a good starting goal to execute. By dropping features we are dealing with fewer data points. Speeds up our notebook and eases the analysis.\n",
    "- Based on our assumptions and decisions we want to drop **'engnat, gender, hand, race, religion, urban, familysize'**  features.\n",
    "- Based on our assumptions and decisions we also want to drop **'Q_Ques', 'Q_Time', 'W_Ques', **  features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 Creating new feature extracting from existing\n",
    "- age, mach, married may be considered to create new feature by banding.\n",
    "\n",
    "- mach_score\n",
    "   - 56점 이상 높아지면 점점 not voted가 많아짐 => classifying\n",
    "- tp_score : 큰 의미 없어보임\n",
    "- age_group + mach_score\n",
    "   - 10s are the most, but most did not vote and they tend to have high mach_score\n",
    "   - 40s, 50s, 60s mostly voted and they tend to have relatively low mach_score\n",
    "   - It seems high mach_score provoke low vote rate and the low is opposite.\n",
    "- Married might divide into two groups at [0.0, 1.0], [2.0, )\n",
    "- Education must be an important feature and is already well-grouped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 Completing a numerical continuous feature (NA)\n",
    "Now we should start estimating and completing features with missing or null values. \n",
    "- familysize seems it has outlier -> but we already eliminated.\n",
    "   \n",
    "We can consider three methods to complete a numerical continuous feature.   \n",
    "1. A simple way is to generate random numbers between mean and [standard deviation](https://en.wikipedia.org/wiki/Standard_deviation).\n",
    "2. More accurate way of guessing missing values is to use other correlated features. In our case we note correlation among Age, Gender, and Pclass. Guess Age values using [median](https://en.wikipedia.org/wiki/Median) values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on...\n",
    "3. Combine methods 1 and 2. So instead of guessing age values based on median, use random numbers between mean and standard deviation, based on sets of Pclass and Gender combinations.\n",
    "\n",
    "### 3.5.4 Completing a categorical feature (NA)\n",
    "- There is no NA.\n",
    "\n",
    "### 3.5.5 Create new feature combining existing features\n",
    "- We created mach_score & tp_score above.\n",
    "- We created mach_age above.\n",
    "\n",
    "### 3.5.6 Converting categorical feature to numeric\n",
    "- age_group\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling, Predict and Solve the problem<a name=\"modeling\"></a>\n",
    "\n",
    "## 4.1 Listing possible model<a name=\"modellisting\"></a>\n",
    "ex)\n",
    "- Randome Forest\n",
    "- KNN\n",
    "\n",
    "## 4.2 Modeling<a name=\"predicting\"></a>\n",
    "### 4.2.1 data setting\n",
    "### 4.2.3 Modeling\n",
    "### 4.2.4 Evaluation(Compare)\n",
    "### 4.2.5 Tuning, Fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1015)\n",
    "# define 'device' to upload tensor in gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/harryjeong/DA/Project/dacon_cup2020'\n",
    "train = pd.read_csv(\"/Users/harryjeong/DA/Project/dacon_cup2020/data/train.csv\", encoding = 'euc-kr')\n",
    "train['DateTime'] = pd.to_datetime(train.DateTime)\n",
    "train['date'] = train.DateTime.dt.date\n",
    "train  = train.groupby('date').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "mini = train.iloc[:,1:].min()\n",
    "size = train.iloc[:,1:].max() - train.iloc[:,1:].min()\n",
    "train.iloc[:,1:] = (train.iloc[:,1:] -  mini) / size\n",
    "\n",
    "input_window = 30\n",
    "output_window = 7\n",
    "\n",
    "window_x = np.zeros((train.shape[0] - (input_window + output_window), input_window, 4))\n",
    "window_y = np.zeros((train.shape[0] - (input_window + output_window), output_window, 4))\n",
    "\n",
    "for start in range(train.shape[0] - (input_window + output_window)):\n",
    "    end = start + input_window    \n",
    "    window_x[start,:, :] = train.iloc[start : end                , 1: ].values\n",
    "    window_y[start,:, :] = train.iloc[end   : end + output_window, 1: ].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size = input_size,\n",
    "                            hidden_size = hidden_size,\n",
    "                            batch_first=True)\n",
    "        self.hidden_lstm = nn.LSTM(input_size = hidden_size,\n",
    "                                   hidden_size = hidden_size,\n",
    "                                   batch_first=True)\n",
    "        \n",
    "        self.time_fc = nn.Linear(hidden_size, 4)\n",
    "    \n",
    "    def forward(self, x_time):\n",
    "    \n",
    "        out_time, _ = self.lstm(x_time)\n",
    "        out_time, _ = self.hidden_lstm(out_time)\n",
    "        out_time, _ = self.hidden_lstm(out_time)\n",
    "        out_time, _ = self.hidden_lstm(out_time)\n",
    "        \n",
    "        \n",
    "        out_time = self.time_fc(out_time[:,-7:, :])\n",
    "        \n",
    "        return out_time.view(-1,7,4)\n",
    "    \n",
    "model = LSTM(input_size = 4, hidden_size = 30).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Epochs train MSE: 0.02835\n",
      "20 Epochs train MSE: 0.01247\n",
      "30 Epochs train MSE: 0.01237\n",
      "40 Epochs train MSE: 0.01013\n",
      "50 Epochs train MSE: 0.00838\n",
      "60 Epochs train MSE: 0.00804\n",
      "70 Epochs train MSE: 0.00792\n",
      "80 Epochs train MSE: 0.00783\n",
      "90 Epochs train MSE: 0.00772\n",
      "100 Epochs train MSE: 0.00760\n",
      "110 Epochs train MSE: 0.00729\n",
      "120 Epochs train MSE: 0.00674\n",
      "130 Epochs train MSE: 0.00643\n",
      "140 Epochs train MSE: 0.00642\n",
      "150 Epochs train MSE: 0.00618\n",
      "160 Epochs train MSE: 0.00609\n",
      "170 Epochs train MSE: 0.00602\n",
      "180 Epochs train MSE: 0.00594\n",
      "190 Epochs train MSE: 0.00585\n",
      "200 Epochs train MSE: 0.00572\n",
      "210 Epochs train MSE: 0.00550\n",
      "220 Epochs train MSE: 0.00711\n",
      "230 Epochs train MSE: 0.00580\n",
      "240 Epochs train MSE: 0.00550\n",
      "250 Epochs train MSE: 0.00525\n",
      "260 Epochs train MSE: 0.00497\n",
      "270 Epochs train MSE: 0.00583\n",
      "280 Epochs train MSE: 0.00503\n",
      "290 Epochs train MSE: 0.00481\n",
      "300 Epochs train MSE: 0.00444\n",
      "310 Epochs train MSE: 0.00408\n",
      "320 Epochs train MSE: 0.00445\n",
      "330 Epochs train MSE: 0.00408\n",
      "340 Epochs train MSE: 0.00350\n",
      "350 Epochs train MSE: 0.00332\n",
      "360 Epochs train MSE: 0.00286\n",
      "370 Epochs train MSE: 0.00262\n",
      "380 Epochs train MSE: 0.00274\n",
      "390 Epochs train MSE: 0.00232\n",
      "400 Epochs train MSE: 0.00209\n",
      "410 Epochs train MSE: 0.00192\n",
      "420 Epochs train MSE: 0.00204\n",
      "430 Epochs train MSE: 0.00180\n",
      "440 Epochs train MSE: 0.00165\n",
      "450 Epochs train MSE: 0.00154\n",
      "460 Epochs train MSE: 0.00150\n",
      "470 Epochs train MSE: 0.00188\n",
      "480 Epochs train MSE: 0.00153\n",
      "490 Epochs train MSE: 0.00153\n"
     ]
    }
   ],
   "source": [
    "window_x = torch.tensor(window_x).float().to(device)\n",
    "window_y = torch.tensor(window_y).float().to(device)\n",
    "\n",
    "# Train model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
    "criterion = nn.MSELoss(size_average = True)\n",
    "num_epochs  = 500\n",
    "train_error = []\n",
    "for t in range(num_epochs):\n",
    "    train_pred = model(window_x)\n",
    "    loss = criterion(train_pred, window_y) ### trend\n",
    "    train_error.append(loss)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if t % 10 == 0 and t !=0:\n",
    "        print(f\"{t} Epochs train MSE: {loss.item():1.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"/Users/harryjeong/DA/Project/dacon_cup2020/data/submission.csv\", encoding = 'euc-kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateTime</th>\n",
       "      <th>사용자</th>\n",
       "      <th>세션</th>\n",
       "      <th>신규방문자</th>\n",
       "      <th>페이지뷰</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>2901</td>\n",
       "      <td>3012</td>\n",
       "      <td>702</td>\n",
       "      <td>69519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-10</td>\n",
       "      <td>2804</td>\n",
       "      <td>2825</td>\n",
       "      <td>660</td>\n",
       "      <td>63647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-11</td>\n",
       "      <td>2712</td>\n",
       "      <td>2710</td>\n",
       "      <td>599</td>\n",
       "      <td>58068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-12</td>\n",
       "      <td>2696</td>\n",
       "      <td>2702</td>\n",
       "      <td>578</td>\n",
       "      <td>58279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-13</td>\n",
       "      <td>2306</td>\n",
       "      <td>2318</td>\n",
       "      <td>479</td>\n",
       "      <td>50695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>2741</td>\n",
       "      <td>2658</td>\n",
       "      <td>627</td>\n",
       "      <td>70617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>2622</td>\n",
       "      <td>2546</td>\n",
       "      <td>606</td>\n",
       "      <td>62725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>2134</td>\n",
       "      <td>2132</td>\n",
       "      <td>460</td>\n",
       "      <td>45885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2021-01-07</td>\n",
       "      <td>2310</td>\n",
       "      <td>2331</td>\n",
       "      <td>555</td>\n",
       "      <td>48447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2021-01-08</td>\n",
       "      <td>1595</td>\n",
       "      <td>1564</td>\n",
       "      <td>316</td>\n",
       "      <td>29287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      DateTime   사용자    세션  신규방문자   페이지뷰\n",
       "0   2020-11-09  2901  3012    702  69519\n",
       "1   2020-11-10  2804  2825    660  63647\n",
       "2   2020-11-11  2712  2710    599  58068\n",
       "3   2020-11-12  2696  2702    578  58279\n",
       "4   2020-11-13  2306  2318    479  50695\n",
       "..         ...   ...   ...    ...    ...\n",
       "56  2021-01-04  2741  2658    627  70617\n",
       "57  2021-01-05  2622  2546    606  62725\n",
       "58  2021-01-06  2134  2132    460  45885\n",
       "59  2021-01-07  2310  2331    555  48447\n",
       "60  2021-01-08  1595  1564    316  29287\n",
       "\n",
       "[61 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(\"/Users/harryjeong/DA/Project/dacon_cup2020/data/submission.csv\", encoding = 'euc-kr')\n",
    "\n",
    "#last_month = torch.tensor(window_x[-1,:,:][np.newaxis,...]).float().to(device) <- 수정 전\n",
    "last_month = train.iloc[-30:,1:].values[np.newaxis,...] # <- 수정 후\n",
    "last_month = torch.tensor(last_month).float().to(device) # <- 수정 후\n",
    "\n",
    "for start in range((len(submission) - output_window)//7 + 2):\n",
    "    start = start * 7\n",
    "    next_week = model(last_month)\n",
    "    #last_month = torch.cat([last_month[-7:], next_week], axis = 1) <- 수정 전\n",
    "    last_month = torch.cat([last_month[:,7:,:], next_week], axis = 1)# <- 수정 후\n",
    "\n",
    "    pred_week = next_week.cpu().detach().numpy().reshape(output_window,4)\n",
    "    pred_week = pred_week * size.values + mini.values\n",
    "    pred_week = pred_week.astype(int)\n",
    "    \n",
    "    if start/7 == (len(submission) - output_window)//7 + 1:\n",
    "        submission.iloc[start :, 1:] = pred_week[-submission.iloc[start :, 1:].shape[0]:,:]\n",
    "    else:\n",
    "        submission.iloc[start : start + output_window, 1:] = pred_week\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index = False, encoding = 'euc-kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
